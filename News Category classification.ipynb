{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1d3b37-abf2-4d01-8fe0-7f494a3db630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "SEED=42\n",
    "torch.manual_seed(SEED)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0240eab-cb1f-4b7a-93ab-b220f69edbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Participants_Data_News_category/Data_Train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3733219-3d8a-4398-97c2-db7b7677631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1, random_state=42, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69260b2e-a179-4822-91dc-7c0980e737f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SECTION = df.SECTION.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_count = np.min(df.SECTION.value_counts())\n",
    "# min_count\n",
    "# df2 = pd.DataFrame(columns=['STORY', 'SECTION'])\n",
    "# texts = []\n",
    "# classes = []\n",
    "# for cat in range(4):\n",
    "#     temp_idxs = df.index[df.SECTION==cat][:min_count]\n",
    "#     frames = [df.iloc[temp_idxs, :], df2]\n",
    "#     df2 = pd.concat(frames, sort=False)\n",
    "\n",
    "# len(df2) / min_count\n",
    "# df2.reset_index(drop = True, inplace=True)\n",
    "# df2.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3157d2f-fcad-4047-800c-f0cc2b5a05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to build vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = \"<UNK>\", mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",end_seq_token=\"<END>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx: Initialize token to idx dictionary.\n",
    "            add_unk: Whether to include the unknown token in the vocabulary\n",
    "            unk_token: How the unknown token is represented in the vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx:token \n",
    "                             for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "#         self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "#         self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        \n",
    "    def from_serializable(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A dictionary that can be serialized\n",
    "        \"\"\"\n",
    "\n",
    "        return cls(**contents)\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        \"\"\" Update mapping dictionaries given the token\n",
    "        Args:\n",
    "            token (str): Token to add to the vocabulary\n",
    "        Returns:\n",
    "            index (int): The index corresponding to the token          \n",
    "        \"\"\"\n",
    "\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "\n",
    "        return index\n",
    "        \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" Retrieves the index associated with the token\n",
    "            or the UNK index if the token isn't present in the vocabulary\n",
    "\n",
    "        Args:\n",
    "            token (str): The token for which the index has to be retrieved\n",
    "        Returns:\n",
    "            index (int): The index associated with the token in the dictionary\n",
    "\n",
    "        Note: \n",
    "            'UNK Index' has to be >=0 for the UNK functionality\n",
    "        \"\"\"\n",
    "\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx.get(token)\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Retrieve the token associated with the index\n",
    "        Args:\n",
    "            index (int): The index to look up\n",
    "        Returns:\n",
    "            token (str): The token associated with the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
    "        else:\n",
    "            return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the vocabulary\n",
    "        \"\"\"\n",
    "        return len(self._token_to_idx)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News_Category(Dataset):\n",
    "    def __init__(self, df, nlp, vocab):\n",
    "        \"\"\"Initializing\n",
    "        Args:\n",
    "            df (Pandas DataFrame): Dataframe consisting of tweets and labels\n",
    "            nlp (spacy object): For preprocessing\n",
    "            vocab (Vocabulary Object): To vectorize the tweets\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.nlp = nlp\n",
    "        self.vocab = vocab\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, self.df.STORY)) + 1\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "          \n",
    "        tweet =  self.df.STORY.iloc[idx]\n",
    "        \n",
    "        label = self.df.SECTION.iloc[idx]\n",
    "        return {'tweet':torch.LongTensor(self.preprocess(tweet)), 'label':label}\n",
    "    \n",
    "    def preprocess(self, sent):\n",
    "        \n",
    "        #Preprocessing and tokenizing\n",
    "        text = sent\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"can not \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "        text = re.sub('\\W', ' ', text)\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        text = text.strip(' ')\n",
    "        text = re.sub('[^a-zA-Z]',' ',text)\n",
    "        text=text.lower()\n",
    "        sent = text\n",
    "        sent =  \" \".join(sent.split())\n",
    "        sent = [token.lemma_ for token in self.nlp(sent) if token.text not in STOP_WORDS]\n",
    "        sent = self.vectorize(sent)\n",
    "        return sent\n",
    "    \n",
    "    def vectorize(self, sent):\n",
    "        \"\"\"Converts raw text to numeric vectors using the vocabulary\n",
    "        Args:\n",
    "            sent (str): The tweet to be vectorized\n",
    "        Returns:\n",
    "            vector (list): The vector associated with the tweet\n",
    "        \"\"\"\n",
    "        vector = [self.vocab.begin_seq_index]\n",
    "#         vector = []\n",
    "        for token in sent:\n",
    "            vector.append(vocab.lookup_token(token))\n",
    "        vector.append(self.vocab.end_seq_index)\n",
    "            \n",
    "        return vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(token_to_idx=None, add_unk=True)\n",
    "nlp = spacy.load(name='en_core_web_sm')\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    text=text.lower()\n",
    "    text = \" \".join(text.split())\n",
    "    text = [token.lemma_ for token in nlp(text) if token.text not in STOP_WORDS]\n",
    "    text = [w for w in text if len(w)>1]\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "painful\n",
      "huge\n",
      "reversal\n",
      "fee\n",
      "income\n",
      "unheard\n",
      "private\n",
      "sector\n",
      "lender\n",
      "essentially\n",
      "mean\n",
      "yes\n",
      "bank\n",
      "take\n",
      "grant\n",
      "fee\n",
      "structured\n",
      "loan\n",
      "deal\n",
      "pay\n",
      "account\n",
      "upfront\n",
      "book\n",
      "borrower\n",
      "turn\n",
      "defaulter\n",
      "fee\n",
      "tie\n",
      "loan\n",
      "deal\n",
      "fall\n",
      "crack\n",
      "gill\n",
      "vow\n",
      "shift\n",
      "safe\n",
      "accounting\n",
      "practice\n",
      "amortize\n",
      "fee\n",
      "income\n",
      "book\n",
      "upfront\n",
      "gill\n",
      "mend\n",
      "past\n",
      "way\n",
      "mean\n",
      "nasty\n",
      "surprise\n",
      "future\n",
      "good\n",
      "news\n",
      "consider\n",
      "investor\n",
      "love\n",
      "clean\n",
      "image\n",
      "loathe\n",
      "uncertainty\n",
      "gain\n",
      "pain\n",
      "promise\n",
      "strong\n",
      "stable\n",
      "balance\n",
      "sheet\n",
      "come\n",
      "sacrifice\n",
      "investor\n",
      "hope\n",
      "phenomenal\n",
      "growth\n",
      "promise\n",
      "kapoor\n"
     ]
    }
   ],
   "source": [
    "for token in clean(df.STORY[0]):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = df.STORY.apply(clean)\n",
    "text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cleaned_tweets)):\n",
    "    for word in cleaned_tweets[i]:\n",
    "        text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436335"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count_dict = Counter(text).most_common(len(set(text))-500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in count_dict:\n",
    "    vocab.add_token(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rerating', 1),\n",
       " ('navigational', 1),\n",
       " ('irns', 1),\n",
       " ('navic', 1),\n",
       " ('known', 1),\n",
       " ('tiranga', 1),\n",
       " ('handlebar', 1),\n",
       " ('likeness', 1),\n",
       " ('takedown', 1),\n",
       " ('frisk', 1),\n",
       " ('frisking', 1),\n",
       " ('demoralise', 1),\n",
       " ('newlywed', 1),\n",
       " ('nuptial', 1),\n",
       " ('karakurissi', 1),\n",
       " ('kalki', 1),\n",
       " ('tyagi', 1),\n",
       " ('reco', 1),\n",
       " ('garrett', 1),\n",
       " ('basch', 1),\n",
       " ('zaillian', 1),\n",
       " ('bron', 1),\n",
       " ('gilbert', 1),\n",
       " ('tethered', 1),\n",
       " ('usmovie', 1),\n",
       " ('dhar', 1),\n",
       " ('ashwatthama', 1),\n",
       " ('annapurna', 1),\n",
       " ('jvm', 1),\n",
       " ('liquidate', 1),\n",
       " ('ei', 1),\n",
       " ('shubham', 1),\n",
       " ('raheja', 1),\n",
       " ('splendor', 1),\n",
       " ('icicidirect', 1),\n",
       " ('bangade', 1),\n",
       " ('incidently', 1),\n",
       " ('maxis', 1),\n",
       " ('symbolism', 1),\n",
       " ('medically', 1),\n",
       " ('sedated', 1),\n",
       " ('naxalism', 1),\n",
       " ('balu', 1),\n",
       " ('hansraj', 1),\n",
       " ('saphi', 1),\n",
       " ('androidtv', 1),\n",
       " ('toggling', 1),\n",
       " ('outback', 1),\n",
       " ('feelthe', 1),\n",
       " ('avengerred', 1),\n",
       " ('skullhugo', 1),\n",
       " ('criminally', 1),\n",
       " ('justified', 1),\n",
       " ('shradha', 1),\n",
       " ('grape', 1),\n",
       " ('erlam', 1),\n",
       " ('utpal', 1),\n",
       " ('bhaskar', 1),\n",
       " ('opensecret', 1),\n",
       " ('org', 1),\n",
       " ('khera', 1),\n",
       " ('unequal', 1),\n",
       " ('upasna', 1),\n",
       " ('mantena', 1),\n",
       " ('induri', 1),\n",
       " ('marg', 1),\n",
       " ('speculative', 1),\n",
       " ('worli', 1),\n",
       " ('peddar', 1),\n",
       " ('gd', 1),\n",
       " ('somani', 1),\n",
       " ('esha', 1),\n",
       " ('doppe', 1),\n",
       " ('pry', 1),\n",
       " ('farsightedness', 1),\n",
       " ('awhile', 1),\n",
       " ('pehna', 1),\n",
       " ('franca', 1),\n",
       " ('hoodie', 1),\n",
       " ('freezing', 1),\n",
       " ('infinite', 1),\n",
       " ('hlt', 1),\n",
       " ('rebuttal', 1),\n",
       " ('hungarian', 1),\n",
       " ('holocaust', 1),\n",
       " ('bogeyman', 1),\n",
       " ('sheryl', 1),\n",
       " ('sandberg', 1),\n",
       " ('mimecast', 1),\n",
       " ('bec', 1),\n",
       " ('parle', 1),\n",
       " ('kundariya', 1),\n",
       " ('kagathara', 1),\n",
       " ('edie', 1),\n",
       " ('zak', 1),\n",
       " ('inventive', 1),\n",
       " ('parisian', 1),\n",
       " ('castelbajac', 1),\n",
       " ('jcc', 1),\n",
       " ('callection', 1),\n",
       " ('elle', 1),\n",
       " ('senegalese', 1),\n",
       " ('maimouna', 1),\n",
       " ('diaye', 1),\n",
       " ('reichardt', 1),\n",
       " ('enki', 1),\n",
       " ('bilal', 1),\n",
       " ('rohrwacher', 1),\n",
       " ('moroccan', 1),\n",
       " ('campillo', 1),\n",
       " ('kasargode', 1),\n",
       " ('gurnani', 1),\n",
       " ('peretti', 1),\n",
       " ('zooming', 1),\n",
       " ('gonzalo', 1),\n",
       " ('noreen', 1),\n",
       " ('krall', 1),\n",
       " ('snyder', 1),\n",
       " ('dunn', 1),\n",
       " ('crutcher', 1),\n",
       " ('smartphonesqualcomm', 1),\n",
       " ('gqualcomm', 1),\n",
       " ('gnew', 1),\n",
       " ('launchesxiaomisamsungapplechina', 1),\n",
       " ('warchina', 1),\n",
       " ('marketiphone', 1),\n",
       " ('nanikanchana', 1),\n",
       " ('uptrade', 1),\n",
       " ('vellaipookalfilm', 1),\n",
       " ('colourist', 1),\n",
       " ('tweetd', 1),\n",
       " ('colorist', 1),\n",
       " ('varusath', 1),\n",
       " ('vivekelan', 1),\n",
       " ('dp', 1),\n",
       " ('jeraldpeter', 1),\n",
       " ('cinemainmygene', 1),\n",
       " ('reviewfilm', 1),\n",
       " ('goutham', 1),\n",
       " ('saravana', 1),\n",
       " ('rajasekar', 1),\n",
       " ('ww', 1),\n",
       " ('filmvarun', 1),\n",
       " ('jerseyvarun', 1),\n",
       " ('sreedhar', 1),\n",
       " ('llaipookal', 1),\n",
       " ('vivekhwide', 1),\n",
       " ('jerseyvennela', 1),\n",
       " ('bhayyoo', 1),\n",
       " ('lenseless', 1),\n",
       " ('blurry', 1),\n",
       " ('steepen', 1),\n",
       " ('kuldeepsinh', 1),\n",
       " ('lohardaga', 1),\n",
       " ('palamu', 1),\n",
       " ('gsp', 1),\n",
       " ('revocation', 1),\n",
       " ('unmuted', 1),\n",
       " ('reliably', 1),\n",
       " ('devlpmnt', 1),\n",
       " ('chandna', 1),\n",
       " ('erasure', 1),\n",
       " ('bitraser', 1),\n",
       " ('kiara', 1),\n",
       " ('kalankvarun', 1),\n",
       " ('zafarin', 1),\n",
       " ('hitting', 1),\n",
       " ('raghu', 1),\n",
       " ('bhuse', 1),\n",
       " ('lit', 1),\n",
       " ('ngemovie', 1),\n",
       " ('bhaveshpunjabi', 1),\n",
       " ('asusual', 1),\n",
       " ('jus', 1),\n",
       " ('kalanksophie', 1),\n",
       " ('tku', 1),\n",
       " ('raiisonai', 1),\n",
       " ('releasekalank', 1),\n",
       " ('reshoot', 1),\n",
       " ('ranaghat', 1),\n",
       " ('potshot', 1),\n",
       " ('speedbreaker', 1),\n",
       " ('khushboo', 1),\n",
       " ('chulha', 1),\n",
       " ('ranjana', 1),\n",
       " ('vanilla', 1),\n",
       " ('bloodshed', 1),\n",
       " ('slogging', 1),\n",
       " ('khyoda', 1),\n",
       " ('apik', 1),\n",
       " ('purno', 1),\n",
       " ('emoting', 1),\n",
       " ('jetliner', 1),\n",
       " ('gugan', 1),\n",
       " ('brajesh', 1),\n",
       " ('goel', 1),\n",
       " ('quashing', 1),\n",
       " ('jaysidheshwar', 1),\n",
       " ('maldive', 1),\n",
       " ('iterative', 1),\n",
       " ('infiltrate', 1),\n",
       " ('barath', 1),\n",
       " ('neelakantan', 1),\n",
       " ('roping', 1),\n",
       " ('courier', 1),\n",
       " ('chana', 1),\n",
       " ('salarie', 1),\n",
       " ('bilaspur', 1),\n",
       " ('alcoholism', 1),\n",
       " ('regularize', 1),\n",
       " ('nationalize', 1),\n",
       " ('mangalam', 1),\n",
       " ('slums', 1),\n",
       " ('ipac', 1),\n",
       " ('ravali', 1),\n",
       " ('kavali', 1),\n",
       " ('brushed', 1),\n",
       " ('chander', 1),\n",
       " ('pondicherry', 1),\n",
       " ('karaikal', 1),\n",
       " ('mahe', 1),\n",
       " ('yanam', 1),\n",
       " ('tranquil', 1),\n",
       " ('permian', 1),\n",
       " ('bruised', 1),\n",
       " ('moat', 1),\n",
       " ('ukranian', 1),\n",
       " ('illia', 1),\n",
       " ('gladshtein', 1),\n",
       " ('neuron', 1),\n",
       " ('lensless', 1),\n",
       " ('prahlad', 1),\n",
       " ('dhillon', 1),\n",
       " ('matebook', 1),\n",
       " ('popularize', 1),\n",
       " ('stre', 1),\n",
       " ('passable', 1),\n",
       " ('slingshot', 1),\n",
       " ('elliptical', 1),\n",
       " ('supermoon', 1),\n",
       " ('bravado', 1),\n",
       " ('purifiersbest', 1),\n",
       " ('purifiersdiwali', 1),\n",
       " ('pollutionair', 1),\n",
       " ('pollutiondiwali', 1),\n",
       " ('pollutiontop', 1),\n",
       " ('purifiersair', 1),\n",
       " ('delhiair', 1),\n",
       " ('blk', 1),\n",
       " ('arafat', 1),\n",
       " ('saiye', 1),\n",
       " ('imprint', 1),\n",
       " ('equidistant', 1),\n",
       " ('arabinda', 1),\n",
       " ('acceleromet', 1),\n",
       " ('mantrayalam', 1),\n",
       " ('thigh', 1),\n",
       " ('misfire', 1),\n",
       " ('mcavoy', 1),\n",
       " ('magneto', 1),\n",
       " ('fassbender', 1),\n",
       " ('dangerously', 1),\n",
       " ('historystar', 1),\n",
       " ('espoo', 1),\n",
       " ('den', 1),\n",
       " ('roposo', 1),\n",
       " ('vigo', 1),\n",
       " ('kwai', 1),\n",
       " ('iike', 1),\n",
       " ('antutu', 1),\n",
       " ('perceptive', 1),\n",
       " ('jetsynthesys', 1),\n",
       " ('pasha', 1),\n",
       " ('asad', 1),\n",
       " ('hayat', 1),\n",
       " ('moreno', 1),\n",
       " ('pfutp', 1),\n",
       " ('adjudicating', 1),\n",
       " ('sawhney', 1),\n",
       " ('mandlik', 1),\n",
       " ('satej', 1),\n",
       " ('sniff', 1),\n",
       " ('descendent', 1),\n",
       " ('chhattrapati', 1),\n",
       " ('decreased', 1),\n",
       " ('advait', 1),\n",
       " ('azadi', 1),\n",
       " ('salahuddin', 1),\n",
       " ('yasin', 1),\n",
       " ('enmity', 1),\n",
       " ('enviable', 1),\n",
       " ('emaciate', 1),\n",
       " ('calculated', 1),\n",
       " ('dandi', 1),\n",
       " ('vallabhbhai', 1),\n",
       " ('bel', 1),\n",
       " ('powley', 1),\n",
       " ('burr', 1),\n",
       " ('warin', 1),\n",
       " ('neal', 1),\n",
       " ('sal', 1),\n",
       " ('villainy', 1),\n",
       " ('asylum', 1),\n",
       " ('siegewritten', 1),\n",
       " ('zemo', 1),\n",
       " ('brutalize', 1),\n",
       " ('bittersweet', 1),\n",
       " ('stricken', 1),\n",
       " ('mangalore', 1),\n",
       " ('petrochemicals', 1),\n",
       " ('surana', 1),\n",
       " ('salwar', 1),\n",
       " ('richey', 1),\n",
       " ('sevens', 1),\n",
       " ('gardens', 1),\n",
       " ('corey', 1),\n",
       " ('duckbill', 1),\n",
       " ('stalemate', 1),\n",
       " ('huawaei', 1),\n",
       " ('hydrawhen', 1),\n",
       " ('helmethank', 1),\n",
       " ('chowdary', 1),\n",
       " ('guatam', 1),\n",
       " ('pritam', 1),\n",
       " ('rawat', 1),\n",
       " ('yashwant', 1),\n",
       " ('fatehpuri', 1),\n",
       " ('husbandry', 1),\n",
       " ('fishery', 1),\n",
       " ('baghel', 1),\n",
       " ('preeta', 1),\n",
       " ('unaffordable', 1),\n",
       " ('cutoutsamsunghonor', 1),\n",
       " ('huaweihonorgalaxy', 1),\n",
       " ('bahia', 1),\n",
       " ('leagold', 1),\n",
       " ('destocking', 1),\n",
       " ('hookup', 1),\n",
       " ('rendering', 1),\n",
       " ('cst', 1),\n",
       " ('forearm', 1),\n",
       " ('ergonomics', 1),\n",
       " ('polyanker', 1),\n",
       " ('carbide', 1),\n",
       " ('zing', 1),\n",
       " ('pcsgamesassemble', 1),\n",
       " ('pcgamersramhard', 1),\n",
       " ('disksstoragegame', 1),\n",
       " ('chipscorsair', 1),\n",
       " ('carbideamd', 1),\n",
       " ('wale', 1),\n",
       " ('unsw', 1),\n",
       " ('explainer', 1),\n",
       " ('sbnation', 1),\n",
       " ('eater', 1),\n",
       " ('howery', 1),\n",
       " ('ferronickel', 1),\n",
       " ('gf', 1),\n",
       " ('wuxi', 1),\n",
       " ('interracial', 1),\n",
       " ('colorful', 1),\n",
       " ('popping', 1),\n",
       " ('umbrellaa', 1),\n",
       " ('marching', 1),\n",
       " ('umbrellas', 1),\n",
       " ('clarkson', 1),\n",
       " ('ella', 1),\n",
       " ('daigle', 1),\n",
       " ('purvi', 1),\n",
       " ('pavitra', 1),\n",
       " ('rishta', 1),\n",
       " ('triumphantly', 1),\n",
       " ('dieterle', 1),\n",
       " ('unpleasant', 1),\n",
       " ('fitzgerald', 1),\n",
       " ('structuralist', 1),\n",
       " ('theorist', 1),\n",
       " ('substitute', 1),\n",
       " ('postmodern', 1),\n",
       " ('pervert', 1),\n",
       " ('testflight', 1),\n",
       " ('gfms', 1),\n",
       " ('johann', 1),\n",
       " ('wiebe', 1),\n",
       " ('generative', 1),\n",
       " ('gan', 1),\n",
       " ('summery', 1),\n",
       " ('avin', 1),\n",
       " ('sputter', 1),\n",
       " ('saturate', 1),\n",
       " ('anshul', 1),\n",
       " ('wishlist', 1),\n",
       " ('myownhouse', 1),\n",
       " ('hariom', 1),\n",
       " ('ganpati', 1),\n",
       " ('bappa', 1),\n",
       " ('maurya', 1),\n",
       " ('seoulrdj', 1),\n",
       " ('repolle', 1),\n",
       " ('neccesary', 1),\n",
       " ('paal', 1),\n",
       " ('plaza', 1),\n",
       " ('ridley', 1),\n",
       " ('rey', 1),\n",
       " ('poe', 1),\n",
       " ('dameron', 1),\n",
       " ('oz', 1),\n",
       " ('atelier', 1),\n",
       " ('prabal', 1),\n",
       " ('gurung', 1),\n",
       " ('heavenly', 1),\n",
       " ('poonawalla', 1),\n",
       " ('forsake', 1),\n",
       " ('mithali', 1),\n",
       " ('veda', 1),\n",
       " ('krishnamurthy', 1),\n",
       " ('jhulan', 1),\n",
       " ('dressing', 1),\n",
       " ('raffish', 1),\n",
       " ('violin', 1),\n",
       " ('chaplinesque', 1),\n",
       " ('flatland', 1),\n",
       " ('jis', 1),\n",
       " ('behti', 1),\n",
       " ('satyam', 1),\n",
       " ('maili', 1),\n",
       " ('aaya', 1),\n",
       " ('pardesi', 1),\n",
       " ('galiyan', 1),\n",
       " ('chaubara', 1),\n",
       " ('studiosjust', 1),\n",
       " ('prithvi', 1),\n",
       " ('khullam', 1),\n",
       " ('khulla', 1),\n",
       " ('neelkanth', 1),\n",
       " ('stabilize', 1),\n",
       " ('saint', 1),\n",
       " ('sanctions', 1),\n",
       " ('pankaja', 1),\n",
       " ('munde', 1),\n",
       " ('pathardi', 1),\n",
       " ('familiarisation', 1),\n",
       " ('plotted', 1),\n",
       " ('alerttony', 1),\n",
       " ('maudlin', 1),\n",
       " ('at', 1),\n",
       " ('zazu', 1),\n",
       " ('eichner', 1),\n",
       " ('rogan', 1),\n",
       " ('meerkat', 1),\n",
       " ('warthog', 1),\n",
       " ('pumbaa', 1),\n",
       " ('diversified', 1),\n",
       " ('bloated', 1),\n",
       " ('miracast', 1),\n",
       " ('licensed', 1),\n",
       " ('unmatched', 1),\n",
       " ('cheddar', 1),\n",
       " ('nba', 1),\n",
       " ('emilia', 1),\n",
       " ('lowe', 1),\n",
       " ('korapaty', 1),\n",
       " ('rba', 1),\n",
       " ('flattener', 1),\n",
       " ('smartphones', 1),\n",
       " ('microchip', 1),\n",
       " ('vyapari', 1),\n",
       " ('renovation', 1),\n",
       " ('forcibly', 1),\n",
       " ('cmp', 1),\n",
       " ('samata', 1),\n",
       " ('manji', 1),\n",
       " ('pacer', 1),\n",
       " ('ganjam', 1),\n",
       " ('kendrapada', 1),\n",
       " ('dali', 1),\n",
       " ('benssalah', 1),\n",
       " ('lashana', 1),\n",
       " ('dencik', 1),\n",
       " ('ana', 1),\n",
       " ('armas', 1),\n",
       " ('fleming', 1),\n",
       " ('bangad', 1),\n",
       " ('sivaganga', 1),\n",
       " ('inx', 1),\n",
       " ('quora', 1),\n",
       " ('purti', 1),\n",
       " ('cars', 1),\n",
       " ('coexist', 1),\n",
       " ('costing', 1),\n",
       " ('cla', 1),\n",
       " ('treason', 1),\n",
       " ('annihilation', 1),\n",
       " ('stair', 1),\n",
       " ('eyewitness', 1),\n",
       " ('armando', 1),\n",
       " ('gil', 1),\n",
       " ('estimated', 1),\n",
       " ('commuting', 1),\n",
       " ('winding', 1),\n",
       " ('townships', 1),\n",
       " ('chamfer', 1),\n",
       " ('hussainpur', 1),\n",
       " ('bopara', 1),\n",
       " ('polarisation', 1),\n",
       " ('tactically', 1),\n",
       " ('raizuddin', 1),\n",
       " ('miserably', 1),\n",
       " ('bhima', 1),\n",
       " ('shyamgiri', 1),\n",
       " ('dandakaranya', 1),\n",
       " ('outcrop', 1),\n",
       " ('megatrend', 1),\n",
       " ('uplink', 1),\n",
       " ('downlink', 1),\n",
       " ('embb', 1),\n",
       " ('digitalization', 1),\n",
       " ('rechristen', 1),\n",
       " ('ndcp', 1),\n",
       " ('bsnl', 1),\n",
       " ('resent', 1),\n",
       " ('miyas', 1),\n",
       " ('nagas', 1),\n",
       " ('sivan', 1),\n",
       " ('buoyant', 1),\n",
       " ('yumi', 1),\n",
       " ('teso', 1),\n",
       " ('yeelight', 1),\n",
       " ('tunable', 1),\n",
       " ('headstart', 1),\n",
       " ('fated', 1),\n",
       " ('kopp', 1),\n",
       " ('etchells', 1),\n",
       " ('tragedy', 1),\n",
       " ('ponder', 1),\n",
       " ('screed', 1),\n",
       " ('conveniently', 1),\n",
       " ('chhatisgarh', 1),\n",
       " ('ghajini', 1),\n",
       " ('seventy', 1),\n",
       " ('bloodshe', 1),\n",
       " ('mainn', 1),\n",
       " ('srinivasula', 1),\n",
       " ('konathala', 1),\n",
       " ('thefateofthefurious', 1),\n",
       " ('tomy', 1),\n",
       " ('kizhakoodan', 1),\n",
       " ('vibe', 1),\n",
       " ('shashank', 1),\n",
       " ('khaitan', 1),\n",
       " ('congratulation', 1),\n",
       " ('choudry', 1),\n",
       " ('shahane', 1),\n",
       " ('jittery', 1),\n",
       " ('biller', 1),\n",
       " ('muralikrishnan', 1),\n",
       " ('comgress', 1),\n",
       " ('duetasha', 1),\n",
       " ('adana', 1),\n",
       " ('performancelegendary', 1),\n",
       " ('tonightshare', 1),\n",
       " ('ashabhosle', 1),\n",
       " ('thevoice', 1),\n",
       " ('starplus', 1),\n",
       " ('endorphin', 1),\n",
       " ('savour', 1),\n",
       " ('kothanodi', 1),\n",
       " ('pastiche', 1),\n",
       " ('thong', 1),\n",
       " ('spotty', 1),\n",
       " ('reschedule', 1),\n",
       " ('negotiating', 1),\n",
       " ('adler', 1),\n",
       " ('lump', 1),\n",
       " ('cowboy', 1),\n",
       " ('swagger', 1),\n",
       " ('khote', 1),\n",
       " ('sikkay', 1),\n",
       " ('yaaro', 1),\n",
       " ('yaar', 1),\n",
       " ('pathan', 1),\n",
       " ('flamboyantly', 1),\n",
       " ('benevolent', 1),\n",
       " ('pran', 1),\n",
       " ('zanjeer', 1),\n",
       " ('seductress', 1),\n",
       " ('unjustly', 1),\n",
       " ('injunctive', 1),\n",
       " ('negativity', 1),\n",
       " ('colombo', 1),\n",
       " ('gruelling', 1),\n",
       " ('incarcerate', 1),\n",
       " ('embalm', 1),\n",
       " ('coaching', 1),\n",
       " ('captaincy', 1),\n",
       " ('receptionist', 1),\n",
       " ('kathryn', 1),\n",
       " ('newton', 1),\n",
       " ('suki', 1),\n",
       " ('waterhouse', 1),\n",
       " ('chaparro', 1),\n",
       " ('geere', 1),\n",
       " ('nighy', 1),\n",
       " ('understatement', 1),\n",
       " ('ruminate', 1),\n",
       " ('pronunciation', 1),\n",
       " ('cazale', 1),\n",
       " ('ethel', 1),\n",
       " ('horne', 1),\n",
       " ('vincente', 1),\n",
       " ('minnelli', 1),\n",
       " ('deference', 1),\n",
       " ('magna', 1),\n",
       " ('jinx', 1),\n",
       " ('raider', 1),\n",
       " ('vijaywada', 1),\n",
       " ('steppe', 1),\n",
       " ('wasteland', 1),\n",
       " ('crafty', 1),\n",
       " ('nomad', 1),\n",
       " ('slaver', 1),\n",
       " ('magician', 1),\n",
       " ('wickedness', 1),\n",
       " ('civiliser', 1),\n",
       " ('improver', 1),\n",
       " ('unpalatable', 1),\n",
       " ('implicitly', 1),\n",
       " ('yunkai', 1),\n",
       " ('astapor', 1),\n",
       " ('mereen', 1),\n",
       " ('inexorable', 1),\n",
       " ('regression', 1),\n",
       " ('barbarism', 1),\n",
       " ('sectarianism', 1),\n",
       " ('squalor', 1),\n",
       " ('imperial', 1),\n",
       " ('redmayne', 1),\n",
       " ('waterston', 1),\n",
       " ('fogler', 1),\n",
       " ('sudol', 1),\n",
       " ('zo', 1),\n",
       " ('kravitz', 1),\n",
       " ('callum', 1),\n",
       " ('claudia', 1),\n",
       " ('nadylam', 1),\n",
       " ('jude', 1),\n",
       " ('johnny', 1),\n",
       " ('depp', 1),\n",
       " ('toby', 1),\n",
       " ('emmerich', 1),\n",
       " ('artistry', 1),\n",
       " ('halle', 1),\n",
       " ('rib', 1),\n",
       " ('wick', 1),\n",
       " ('parabellum', 1),\n",
       " ('sofia', 1),\n",
       " ('keanu', 1),\n",
       " ('amaal', 1),\n",
       " ('mila', 1),\n",
       " ('haina', 1),\n",
       " ('vaddi', 1),\n",
       " ('sharaban', 1),\n",
       " ('khanwhile', 1),\n",
       " ('mayanagri', 1),\n",
       " ('liberally', 1),\n",
       " ('ameya', 1),\n",
       " ('petulant', 1),\n",
       " ('tells', 1),\n",
       " ('crisply', 1),\n",
       " ('kurtas', 1),\n",
       " ('sleeved', 1),\n",
       " ('plum', 1),\n",
       " ('dhisco', 1),\n",
       " ('hosts', 1),\n",
       " ('aadhi', 1),\n",
       " ('pinisetty', 1),\n",
       " ('abate', 1),\n",
       " ('sutariabharti', 1),\n",
       " ('teacherdure', 1),\n",
       " ('sooraj', 1),\n",
       " ('pancholi', 1),\n",
       " ('janaury', 1),\n",
       " ('beige', 1),\n",
       " ('ecstatic', 1),\n",
       " ('sible', 1),\n",
       " ('whitestone', 1),\n",
       " ('catalog', 1),\n",
       " ('replying', 1),\n",
       " ('afterlife', 1),\n",
       " ('will', 1),\n",
       " ('resentment', 1),\n",
       " ('protecture', 1),\n",
       " ('pseudonym', 1),\n",
       " ('techmnxt', 1),\n",
       " ('charter', 1),\n",
       " ('unnati', 1),\n",
       " ('saale', 1),\n",
       " ('bobbeeta', 1),\n",
       " ('wb', 1),\n",
       " ('alertnessscanner', 1),\n",
       " ('cornell', 1),\n",
       " ('even', 1),\n",
       " ('ramjan', 1),\n",
       " ('unreasonable', 1),\n",
       " ('precondition', 1),\n",
       " ('bawana', 1),\n",
       " ('brijesh', 1),\n",
       " ('costco', 1),\n",
       " ('proportionate', 1),\n",
       " ('shouvik', 1),\n",
       " ('rudely', 1),\n",
       " ('bluehole', 1),\n",
       " ('pari', 1),\n",
       " ('passu', 1),\n",
       " ('greedy', 1),\n",
       " ('alan', 1),\n",
       " ('maafinama', 1),\n",
       " ('unconstitutional', 1),\n",
       " ('treachery', 1),\n",
       " ('trending', 1),\n",
       " ('lescure', 1),\n",
       " ('frmaux', 1),\n",
       " ('silverscreen', 1),\n",
       " ('yacht', 1),\n",
       " ('quince', 1),\n",
       " ('kusumavathi', 1),\n",
       " ('chikkanagoudara', 1),\n",
       " ('erratic', 1),\n",
       " ('alistair', 1),\n",
       " ('hewitt', 1),\n",
       " ('mapo', 1),\n",
       " ('grader', 1),\n",
       " ('harlem', 1),\n",
       " ('classmate', 1),\n",
       " ('baret', 1),\n",
       " ('internship', 1),\n",
       " ('parenting', 1),\n",
       " ('jnu', 1),\n",
       " ('kirron', 1),\n",
       " ('sikander', 1),\n",
       " ('laaxmibomb', 1),\n",
       " ('filmmakers', 1),\n",
       " ('madhur', 1),\n",
       " ('bhandarkar', 1),\n",
       " ('axon', 1),\n",
       " ('adieu', 1),\n",
       " ('watts', 1),\n",
       " ('macfarlane', 1),\n",
       " ('sienna', 1),\n",
       " ('apocalypse', 1),\n",
       " ('marunga', 1),\n",
       " ('sly', 1),\n",
       " ('hisaab', 1),\n",
       " ('lenge', 1),\n",
       " ('crate', 1),\n",
       " ('drool', 1),\n",
       " ('orchard', 1),\n",
       " ('mishti', 1),\n",
       " ('mishtanna', 1),\n",
       " ('bhandar', 1),\n",
       " ('dwight', 1),\n",
       " ('morrow', 1),\n",
       " ('scrollable', 1),\n",
       " ('stayed', 1),\n",
       " ('ariana', 1),\n",
       " ('grande', 1),\n",
       " ('malone', 1),\n",
       " ('travis', 1),\n",
       " ('mgm', 1),\n",
       " ('scorpion', 1),\n",
       " ('vaughn', 1),\n",
       " ('unsupervised', 1),\n",
       " ('probation', 1),\n",
       " ('discountpoco', 1),\n",
       " ('flipkartflipkartpoco', 1),\n",
       " ('reviewflipkart', 1),\n",
       " ('tenebris', 1),\n",
       " ('burbank', 1),\n",
       " ('auctioneer', 1),\n",
       " ('bonhams', 1),\n",
       " ('bom', 1),\n",
       " ('noticee', 1),\n",
       " ('governing', 1),\n",
       " ('deception', 1),\n",
       " ('decoy', 1),\n",
       " ('nist', 1),\n",
       " ('hitrust', 1),\n",
       " ('pci', 1),\n",
       " ('dss', 1),\n",
       " ('anonymize', 1),\n",
       " ('changele', 1),\n",
       " ('fo', 1),\n",
       " ('rainy', 1),\n",
       " ('weatherproof', 1),\n",
       " ('polyester', 1),\n",
       " ('afar', 1),\n",
       " ('poker', 1),\n",
       " ('pub', 1),\n",
       " ('localize', 1),\n",
       " ('supercell', 1),\n",
       " ('lifeblood', 1),\n",
       " ('fulll', 1),\n",
       " ('varundvn', 1),\n",
       " ('questions', 1),\n",
       " ('midsize', 1),\n",
       " ('scape', 1),\n",
       " ('toshimitsu', 1),\n",
       " ('motegi', 1),\n",
       " ('henge', 1),\n",
       " ('blasphemy', 1),\n",
       " ('satan', 1),\n",
       " ('rdb', 1),\n",
       " ('omcs', 1),\n",
       " ('angelo', 1),\n",
       " ('speakersnew', 1),\n",
       " ('timesthe', 1),\n",
       " ('podcastnyt', 1),\n",
       " ('podcastmichael', 1),\n",
       " ('barbaroalexa', 1),\n",
       " ('newstechnology', 1),\n",
       " ('outclass', 1),\n",
       " ('generational', 1),\n",
       " ('encyclopedia', 1),\n",
       " ('britannica', 1),\n",
       " ('wothadei', 1),\n",
       " ('expeditious', 1),\n",
       " ('sonu', 1),\n",
       " ('gopkumar', 1),\n",
       " ('sterilize', 1),\n",
       " ('pahadi', 1),\n",
       " ('shareef', 1),\n",
       " ('nowgam', 1),\n",
       " ('qazigund', 1),\n",
       " ('foothill', 1),\n",
       " ('gujjar', 1),\n",
       " ('peoples', 1),\n",
       " ('husnain', 1),\n",
       " ('masoodi', 1),\n",
       " ('pessimist', 1),\n",
       " ('manger', 1),\n",
       " ('petersburg', 1),\n",
       " ('ahai', 1),\n",
       " ('sandor', 1),\n",
       " ('gregor', 1),\n",
       " ('recoverable', 1),\n",
       " ('deregister', 1),\n",
       " ('overwrite', 1),\n",
       " ('bijlani', 1),\n",
       " ('erfan', 1),\n",
       " ('paradeep', 1),\n",
       " ('samal', 1),\n",
       " ('spent', 1),\n",
       " ('butta', 1),\n",
       " ('forty', 1),\n",
       " ('dube', 1),\n",
       " ('disillusionment', 1),\n",
       " ('crossfire', 1),\n",
       " ('collateral', 1),\n",
       " ('heckle', 1),\n",
       " ('dem', 1),\n",
       " ('hoffman', 1),\n",
       " ('atkinson', 1),\n",
       " ('mississippi', 1),\n",
       " ('moran', 1),\n",
       " ('blumenthal', 1),\n",
       " ('schatz', 1),\n",
       " ('hawaii', 1),\n",
       " ('eyewear', 1),\n",
       " ('psychotic', 1),\n",
       " ('cohn', 1),\n",
       " ('missus', 1),\n",
       " ('hypercompetitive', 1),\n",
       " ('maxwell', 1),\n",
       " ('folger', 1),\n",
       " ('paas', 1),\n",
       " ('debjani', 1),\n",
       " ('reckoner', 1),\n",
       " ('thampi', 1),\n",
       " ('aastha', 1),\n",
       " ('rawal', 1),\n",
       " ('frontrunner', 1),\n",
       " ('sella', 1),\n",
       " ('nevo', 1),\n",
       " ('smulder', 1),\n",
       " ('kaal', 1),\n",
       " ('parkash', 1),\n",
       " ('filmnation', 1),\n",
       " ('misanthrope', 1),\n",
       " ('dreamy', 1),\n",
       " ('shinoda', 1),\n",
       " ('delson', 1),\n",
       " ('farrell', 1),\n",
       " ('bourdon', 1),\n",
       " ('bennington', 1),\n",
       " ('downwardly', 1),\n",
       " ('iip', 1),\n",
       " ('intension', 1),\n",
       " ('analog', 1),\n",
       " ('safelet', 1),\n",
       " ('bracelet', 1),\n",
       " ('foreo', 1),\n",
       " ('cleansing', 1),\n",
       " ('exfoliate', 1),\n",
       " ('pulsation', 1),\n",
       " ('drying', 1),\n",
       " ('styling', 1),\n",
       " ('airflow', 1),\n",
       " ('assiduously', 1),\n",
       " ('profusion', 1),\n",
       " ('embolden', 1),\n",
       " ('borne', 1),\n",
       " ('governmental', 1),\n",
       " ('asteroid', 1),\n",
       " ('lenskart', 1),\n",
       " ('spriha', 1),\n",
       " ('attero', 1),\n",
       " ('suppression', 1),\n",
       " ('zilla', 1),\n",
       " ('zptc', 1),\n",
       " ('mptc', 1),\n",
       " ('ceat', 1),\n",
       " ('coe', 1),\n",
       " ('jignesh', 1),\n",
       " ('mewani', 1),\n",
       " ('wearesocial', 1),\n",
       " ('varrier', 1),\n",
       " ('counterintuitive', 1),\n",
       " ('diffusion', 1),\n",
       " ('chowhdury', 1),\n",
       " ('baahubalimovie', 1),\n",
       " ('afinsky', 1),\n",
       " ('geriatric', 1),\n",
       " ('luddite', 1),\n",
       " ('shorten', 1),\n",
       " ('roma', 1),\n",
       " ('regal', 1),\n",
       " ('moctezuma', 1),\n",
       " ('animaniacs', 1),\n",
       " ('pittsburgh', 1),\n",
       " ('michelin', 1),\n",
       " ('reviewhitmansilent', 1),\n",
       " ('assassinps', 1),\n",
       " ('xboxps', 1),\n",
       " ('xboxgamingtechnology', 1),\n",
       " ('happinessbegin', 1),\n",
       " ('sucker', 1),\n",
       " ('leijun', 1),\n",
       " ('begging', 1),\n",
       " ('hamne', 1),\n",
       " ('hekdi', 1),\n",
       " ('nika', 1),\n",
       " ('katora', 1),\n",
       " ('leke', 1),\n",
       " ('dunia', 1),\n",
       " ('ghumne', 1),\n",
       " ('mene', 1),\n",
       " ('samadhi', 1),\n",
       " ('ebook', 1),\n",
       " ('deborshi', 1),\n",
       " ('chaki', 1),\n",
       " ('mehzabin', 1),\n",
       " ('indpendent', 1),\n",
       " ('gotten', 1),\n",
       " ('survivalist', 1),\n",
       " ('unpopulated', 1),\n",
       " ('shoshone', 1),\n",
       " ('wyoming', 1),\n",
       " ('connexxa', 1),\n",
       " ('installable', 1),\n",
       " ('nytime', 1),\n",
       " ('dsf', 1),\n",
       " ('wma', 1),\n",
       " ('khz', 1),\n",
       " ('subvert', 1),\n",
       " ('trope', 1),\n",
       " ('avatars', 1),\n",
       " ('grp', 1),\n",
       " ('phonetic', 1),\n",
       " ('kathi', 1),\n",
       " ('toka', 1),\n",
       " ('mpcs', 1),\n",
       " ('thromwa', 1),\n",
       " ('konyak', 1),\n",
       " ('restate', 1),\n",
       " ('kunchacko', 1),\n",
       " ('boban', 1),\n",
       " ('sreenath', 1),\n",
       " ('bhasi', 1),\n",
       " ('rima', 1),\n",
       " ('kallingal', 1),\n",
       " ('remya', 1),\n",
       " ('nambeesan', 1),\n",
       " ('joju', 1),\n",
       " ('dileesh', 1),\n",
       " ('pothan', 1),\n",
       " ('senthil', 1),\n",
       " ('revathy', 1),\n",
       " ('kelunni', 1),\n",
       " ('thiruvothu', 1),\n",
       " ('indran', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = News_Category(df,nlp, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweet': tensor([    1, 20449, 24527, 15394, 24220, 20717, 16024,  7911, 17317, 13577,\n",
       "          8647,     0, 10105, 15542, 11452, 15394, 14100, 12210, 17336, 21340,\n",
       "          1144, 21536, 20559, 18065, 22543,  7102, 15394,  5660,  4034,  3207,\n",
       "         16698, 17705,  4034,  2891,  4713, 23606,  1443,  6036,   169, 14175,\n",
       "         14055, 14584, 20085, 12113,  2213,  1833, 22774,  4510,  1351,  3811,\n",
       "          9498,  7718,  1025, 24575, 14584, 13931,     2]),\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beginner'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_index(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(filepath):\n",
    "    \"\"\"Loads the glove embeddings\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): path to the glove embeddings file\n",
    "    Return:\n",
    "        word_to_index (dict): Mappings from word to index\n",
    "        embeddings (np.array): Embeddings of the words in the vocabulary\n",
    "    \"\"\"\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \")\n",
    "            word_to_index[line[0]] = index\n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    \n",
    "    return word_to_index, np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(filepath, words):\n",
    "    \"\"\"Create embedding matrix for a specific set of words\n",
    "    Args:\n",
    "        word_to_index (dict) : mapping of word to index\n",
    "        embeddings (list): embeddings of words\n",
    "        words (list): List of words in the dictionary\n",
    "    Returns:\n",
    "        final_embeddings (np..array) : embedding matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    word_to_idx, embeddings = load_glove(filepath)\n",
    "    embedding_size = embeddings.shape[1]\n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.zeros(embedding_size)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "            \n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for idx in range(0, vocab.__len__()):\n",
    "    words.append(vocab.lookup_index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = make_embedding_matrix(r\"C:\\Users\\win10\\Documents\\glove.6B\\glove.6B.300d.txt\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nullah', (300,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=500\n",
    "words[i], embs[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([    1, 21507,  4780, 10259, 22374, 16099, 13212, 10903, 10034, 10188,\n",
       "          1128, 23584,  6083, 17023, 22395,  5363, 14999, 22649,   831,  2676,\n",
       "         22649,  7010, 11023,  6185,  6810, 22738,  6083,     0, 12490,  3440,\n",
       "         17486,  7766, 14999,  3392, 10754, 18956, 12846,  8133, 22738,  1128,\n",
       "         23584,  2519, 13212,  4847,     2]),\n",
       " 0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(0,4200)\n",
    "print(i)\n",
    "data[i]['tweet'], data[i]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, label_size, batch_size, embedding_weights, num_layers = 1,bidirectional = False):\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.batch_size = batch_size\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights, freeze=False, padding_idx=0)\n",
    "#         self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim,\n",
    "#                             num_layers = num_layers, bidirectional = bidirectional,batch_first=True)\n",
    "#         if bidirectional:\n",
    "#             self.fc = nn.Linear(hidden_dim*2, label_size)\n",
    "#         else:\n",
    "#             self.fc = nn.Linear(hidden_dim, label_size)\n",
    " \n",
    "#     def forward(self, sentences, train = True):\n",
    "#         embeds = self.word_embeddings(sentences)\n",
    "#         packed_outputs, (hidden,cell) = self.lstm(embeds)\n",
    "#         dense_outputs = self.fc(hidden[1])\n",
    "#         outputs = dense_outputs\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size, embedding_weights, num_layers = 1,bidirectional = False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.embedding_dim, num_heads=1)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights, freeze=False, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size = self.embedding_dim, hidden_size = hidden_dim,\n",
    "                            num_layers = num_layers, bidirectional = bidirectional,batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, label_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, label_size)\n",
    " \n",
    "    def forward(self, sentences, train = True):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        embeds = embeds.permute(1,0,2)\n",
    "        attn_output, _ = self.attention(embeds, embeds, embeds)\n",
    "        packed_outputs, (hidden,cell) = self.lstm(attn_output.permute(1,0,2))\n",
    "        dense_outputs = self.fc(hidden[1])\n",
    "        outputs = dense_outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_nn = nn.LSTM(input_size = 100, hidden_size = 100,\n",
    "#                 num_layers = 2, bidirectional = False,batch_first=True)\n",
    "# print(test_nn(torch.rand(32,8,100))[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "nlabel = 4\n",
    "num_layers = 2\n",
    "hidden_dim = 512\n",
    "EMBEDDING_DIM = embs.shape[1]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = LSTMClassifier(embedding_dim=EMBEDDING_DIM,hidden_dim=hidden_dim,label_size=nlabel, batch_size=BATCH_SIZE, embedding_weights=torch.from_numpy(embs).float(), num_layers=num_layers)\n",
    "model = model.to(device)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    " \n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewrite collate_ FN function, whose input is the sample data of a batch\n",
    "def collate_fn(batch):\n",
    "\t#Because token_ List is a variable length data, so you need to use a list to load the token of the batch_ list\n",
    "    token_lists = [item['tweet'] for item in batch]\n",
    "    #Each label is an int. we take out all the labels in the batch and reassemble them\n",
    "    labels = [item['label'] for item in batch]\n",
    "    #Converting labels to tensor\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return {\n",
    "    'token_list': torch.nn.utils.rnn.pad_sequence(token_lists, batch_first=True),\n",
    "    'label': labels,\n",
    "    }\n",
    "\n",
    "#When using dataloader to load data, pay attention to collate_ The FN parameter passes in an overridden function\n",
    "trainset = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 21327, 15011,  ...,     0,     0,     0],\n",
      "        [    1,  9455,  4973,  ...,     0,     0,     0],\n",
      "        [    1, 21214, 13289,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    1, 23685,   441,  ...,     0,     0,     0],\n",
      "        [    1,   159, 10999,  ...,     0,     0,     0],\n",
      "        [    1, 19429,  1598,  ...,     0,     0,     0]])\n",
      "tensor([1, 0, 2, 1, 2, 3, 3, 2, 2, 3, 1, 0, 2, 0, 0, 0, 1, 2, 2, 2, 3, 1, 2, 1,\n",
      "        1, 1, 1, 0, 1, 2, 2, 2, 0, 3, 2, 2, 1, 1, 3, 1, 0, 2, 3, 1, 2, 0, 1, 0,\n",
      "        0, 2, 2, 1, 1, 2, 1, 2, 0, 2, 0, 2, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainset:\n",
    "    print(batch['token_list'])\n",
    "    print(batch['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:40<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1 = 0.47069135028868914\n",
      "accuracy on epoch 1 = 0.8333767364422481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:49<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 2 = 0.12602893096239617\n",
      "accuracy on epoch 2 = 0.9631076390544574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:47<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 3 = 0.08178024136771758\n",
      "accuracy on epoch 3 = 0.97734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:27<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 4 = 0.04905979229467145\n",
      "accuracy on epoch 4 = 0.9861979166666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:20<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 5 = 0.04614558935863897\n",
      "accuracy on epoch 5 = 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:22<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 6 = 0.03525848129259733\n",
      "accuracy on epoch 6 = 0.9896701390544573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:44<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 7 = 0.018339037647335014\n",
      "accuracy on epoch 7 = 0.9947916666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:46<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 8 = 0.01702782964760748\n",
      "accuracy on epoch 8 = 0.9954427083333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:30<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 9 = 0.01573876737966202\n",
      "accuracy on epoch 9 = 0.9954427083333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:32<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 10 = 0.011045540863657758\n",
      "accuracy on epoch 10 = 0.9962239583333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:34<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 11 = 0.010523825896598282\n",
      "accuracy on epoch 11 = 0.9967447916666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:38<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 12 = 0.011178847795478456\n",
      "accuracy on epoch 12 = 0.9951388890544574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:40<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 13 = 0.006773110715039365\n",
      "accuracy on epoch 13 = 0.997265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:40<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 14 = 0.005657874824919418\n",
      "accuracy on epoch 14 = 0.9971354166666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:33<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 15 = 0.0057699566335638035\n",
      "accuracy on epoch 15 = 0.9977864583333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:40<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 16 = 0.005201868737534217\n",
      "accuracy on epoch 16 = 0.9973958333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:50<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 17 = 0.004857403583431127\n",
      "accuracy on epoch 17 = 0.997265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:40<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 18 = 0.004576163718153717\n",
      "accuracy on epoch 18 = 0.9975260416666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:38<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 19 = 0.004783821440863297\n",
      "accuracy on epoch 19 = 0.997265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:31<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 20 = 0.0056270947432039975\n",
      "accuracy on epoch 20 = 0.9971354166666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:30<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 21 = 0.005265184736572337\n",
      "accuracy on epoch 21 = 0.9970052083333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:31<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 22 = 0.004242449122360389\n",
      "accuracy on epoch 22 = 0.9985677083333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:30<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 23 = 0.004865084525939286\n",
      "accuracy on epoch 23 = 0.9975260416666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:30<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 24 = 0.004625556626729121\n",
      "accuracy on epoch 24 = 0.9975260416666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 120/120 [02:30<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 25 = 0.004508763731723775\n",
      "accuracy on epoch 25 = 0.9973958333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs=25\n",
    "for epoch in range(epochs):\n",
    "    time.sleep(1)\n",
    "    total_loss = 0.0\n",
    "    total_acc=0.0\n",
    "    for i, batch in enumerate(tqdm(trainset)):\n",
    "        feature, label = batch['token_list'].to(device), batch['label'].to(device)\n",
    "#         batch_length = torch.tensor(33, dtype = torch.int64).unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        output =  model(feature).squeeze()\n",
    "        loss = loss_function(output, label)\n",
    "        acc=categorical_accuracy(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item() \n",
    "        \n",
    "    scheduler.step()    \n",
    "    print(f\"loss on epoch {epoch+1} = {total_loss/len(trainset)}\")\n",
    "    print(f\"accuracy on epoch {epoch+1} = {total_acc/len(trainset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_excel('Participants_Data_News_category/Sample_submission.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_excel('Participants_Data_News_category\\Data_Test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(model, text, min_len = 4):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    text=text.lower()\n",
    "    text = \" \".join(text.split())\n",
    "    text = [token.lemma_ for token in nlp(text) if token.text not in STOP_WORDS]\n",
    "    vector = []\n",
    "    for token in text:\n",
    "        vector.append(vocab.lookup_token(token))\n",
    "    tensor = torch.LongTensor(vector)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    preds = model(tensor.to(device))\n",
    "    pred_class = preds.argmax(dim = 1)\n",
    "    return pred_class[0]\n",
    "#     print('The sentence is : {}'.format(sent))\n",
    "    # print(f'Predicted class is: {pred_class.item()} = {itol[int(pred_class)]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2748/2748 [01:01<00:00, 44.82it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "for idx in tqdm(test_set.index):\n",
    "    preds.append(predict_class(model,test_set.loc[idx, 'STORY']).item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10472/2761623411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubmission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'submission' is not defined"
     ]
    }
   ],
   "source": [
    "# submission.to_excel('submission.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds, columns=['SECTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SECTION\n",
       "0           1\n",
       "1           1\n",
       "2           1\n",
       "3           1\n",
       "4           1\n",
       "...       ...\n",
       "2743        1\n",
       "2744        1\n",
       "2745        1\n",
       "2746        0\n",
       "2747        1\n",
       "\n",
       "[2748 rows x 1 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(submission.SECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'SECTION'}>]], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gadget',\n",
       " 'like',\n",
       " 'gaming',\n",
       " 'smartphone',\n",
       " 'wearable',\n",
       " 'medical',\n",
       " 'device',\n",
       " 'lift',\n",
       " 'user',\n",
       " 'experience',\n",
       " 'new',\n",
       " 'level',\n",
       " 'mint',\n",
       " 'india',\n",
       " 'wire',\n",
       " 'consumer',\n",
       " 'technologyconsumer',\n",
       " 'technology',\n",
       " 'trend',\n",
       " 'new',\n",
       " 'yeartech',\n",
       " 'gadgetsfoldable',\n",
       " 'phonesgame',\n",
       " 'smartphoneswearable',\n",
       " 'medical',\n",
       " 'devicestechnology',\n",
       " 'new',\n",
       " 'delhi',\n",
       " 'gadget',\n",
       " 'integral',\n",
       " 'life',\n",
       " 'rely',\n",
       " 'form',\n",
       " 'factor',\n",
       " 'communicate',\n",
       " 'commute',\n",
       " 'work',\n",
       " 'inform',\n",
       " 'entertain',\n",
       " 'year',\n",
       " 'gadget',\n",
       " 'lift',\n",
       " 'user',\n",
       " 'experience',\n",
       " 'new',\n",
       " 'level',\n",
       " 'expect',\n",
       " 'smartphone',\n",
       " 'foldable',\n",
       " 'screen',\n",
       " 'foldable',\n",
       " 'phone',\n",
       " 'finally',\n",
       " 'move',\n",
       " 'concept',\n",
       " 'stage',\n",
       " 'commercial',\n",
       " 'launch',\n",
       " 'organic',\n",
       " 'light',\n",
       " 'emit',\n",
       " 'diode',\n",
       " 'ole',\n",
       " 'panel',\n",
       " 'high',\n",
       " 'plastic',\n",
       " 'substrate',\n",
       " 'allow',\n",
       " 'bent',\n",
       " 'damage',\n",
       " 'base',\n",
       " 'display',\n",
       " 'maker',\n",
       " 'royole',\n",
       " 'corp',\n",
       " 'foldable',\n",
       " 'phone',\n",
       " 'flexpai',\n",
       " 'arrive',\n",
       " 'select',\n",
       " 'market',\n",
       " 'samsung',\n",
       " 'unnamed',\n",
       " 'foldable',\n",
       " 'phone',\n",
       " 'expect',\n",
       " 'year',\n",
       " 'samsung',\n",
       " 'smartphone',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'officer',\n",
       " 'koh',\n",
       " 'say',\n",
       " 'million',\n",
       " 'unit',\n",
       " 'lg',\n",
       " 'expect',\n",
       " 'display',\n",
       " 'foldable',\n",
       " 'phone',\n",
       " 'year',\n",
       " 'apple',\n",
       " 'nokia',\n",
       " 'lenovo',\n",
       " 'huawei',\n",
       " 'work',\n",
       " 'foldable',\n",
       " 'phone',\n",
       " 'reportedly',\n",
       " 'esim',\n",
       " 'soon',\n",
       " 'smartphone',\n",
       " 'win',\n",
       " 'need',\n",
       " 'physical',\n",
       " 'sim',\n",
       " 'card',\n",
       " 'anymore',\n",
       " 'esim',\n",
       " 'technology',\n",
       " 'apple',\n",
       " 'iphone',\n",
       " 'apple',\n",
       " 'watch',\n",
       " 'replace',\n",
       " 'physical',\n",
       " 'sim',\n",
       " 'virtually',\n",
       " 'embed',\n",
       " 'chip',\n",
       " 'motherboard',\n",
       " 'esim',\n",
       " 'support',\n",
       " 'multiple',\n",
       " 'mobile',\n",
       " 'operator',\n",
       " 'program',\n",
       " 'switch',\n",
       " 'service']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(test_set.STORY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019 will see gadgets like gaming smartphones and wearable medical devices lifting the user experience to a whole new level\\n\\n\\nmint-india-wire consumer technologyconsumer technology trends in New Yeartech gadgetsFoldable phonesgaming smartphoneswearable medical devicestechnology\\n\\n\\nNew Delhi: Gadgets have become an integral part of our lives with most of us relying on some form of factor to communicate, commute, work, be informed or entertained. Year 2019 will see some gadgets lifting the user experience to a whole new level. Heres what we can expect to see:\\n\\n\\nSmartphones with foldable screens: Foldable phones are finally moving from the concept stage to commercial launches. They are made up of organic light-emitting diode (OLED) panels with higher plastic substrates, allowing them to be bent without damage.\\n\\n\\nUS-based display maker Royole Corps foldable phone, FlexPai, has already arrived in select markets, while Samsungs unnamed foldable phone is expected sometime next year. Samsungs smartphone chief executive officer D.J. Koh has said they will make a million units of it. LG, too, is expected to display a foldable phone next year. Meanwhile Apple, Nokia, Lenovo and Huawei have also been working on foldable phones, reportedly.\\n\\n\\neSIM: Very soon your smartphone wont need a physical SIM card anymore. The eSIM technology, already used by Apple in its iPhones and Apple Watch, replaces the physical SIM with a virtually embedded chip on the motherboard. eSIMs support multiple mobile operators and can be programmed to switch services.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.STORY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9d77b0e9f9ca67815d5a1c7db8aa306d48fa5c14152701c5d5c5c11f6164f8b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
